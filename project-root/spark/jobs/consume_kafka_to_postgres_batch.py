from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# 1ï¸âƒ£ Kafka ë° Postgres í™˜ê²½ì„¤ì •
KAFKA_BROKERS = "kafka1:29092,kafka2:29093,kafka3:29094"  # ë˜ëŠ” localhost:9092,9093,9094
# KAFKA_BROKERS = "localhost:9092,localhost:9093,localhost:9094"
TOPIC_NAME = "user_events"

POSTGRES_URL = "jdbc:postgresql://postgres:5432/airflow"
POSTGRES_USER = "airflow"
POSTGRES_PW = "airflow"
POSTGRES_TABLE = "user_events_stream"

# 2ï¸âƒ£ SparkSession ìƒì„±
spark = SparkSession.builder \
    .appName("KafkaToPostgresBatch") \
    .config(
        "spark.jars.packages",
        "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,"
        "org.postgresql:postgresql:42.6.0"
    ) \
    .getOrCreate()
    # .config("spark.jars.packages", "org.postgresql:postgresql:42.6.0") \
    # .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1") \

spark.sparkContext.setLogLevel("WARN")

# 3ï¸âƒ£ Kafkaì—ì„œ ë©”ì‹œì§€ ì½ê¸° (ë°°ì¹˜ëª¨ë“œ)
df_raw = spark.read \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BROKERS) \
    .option("subscribe", TOPIC_NAME) \
    .option("startingOffsets", "earliest") \
    .load()

df_raw = df_raw.selectExpr("CAST(value AS STRING) as json_str")

# 4ï¸âƒ£ JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = StructType([
    StructField("user_id", IntegerType(), True),
    StructField("event", StringType(), True),
    StructField("amount", IntegerType(), True),
    StructField("page", StringType(), True),
    StructField("timestamp", StringType(), True)
])

# 5ï¸âƒ£ JSON íŒŒì‹±
df_parsed = df_raw \
    .select(from_json(col("json_str"), schema).alias("data")) \
    .select("data.*") \
    .withColumnRenamed("timestamp", "event_time")

# 6ï¸âƒ£ ì½˜ì†” ì¶œë ¥
print("ğŸ“¦ Kafka ë©”ì‹œì§€ ì¶œë ¥:")
df_parsed.show(truncate=False)

# 6ï¸âƒ£ PostgreSQLì— ì €ì¥ (í…Œì´ë¸” ë§¤ë²ˆ ë®ì–´ì“°ê¸°)
df_parsed.write \
    .format("jdbc") \
    .option("url", POSTGRES_URL) \
    .option("dbtable", POSTGRES_TABLE) \
    .option("user", POSTGRES_USER) \
    .option("password", POSTGRES_PW) \
    .option("driver", "org.postgresql.Driver") \
    .mode("overwrite") \
    .save()

print(f"âœ… PostgreSQL({POSTGRES_TABLE})ì— Kafka ë©”ì‹œì§€ {df_parsed.count()}ê±´ ì €ì¥ ì™„ë£Œ.")
spark.stop()
